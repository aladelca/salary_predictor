{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import re\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"data/salary_data_cleaned.csv\")\n",
    "data.head(10)\n"
   ],
   "id": "443ec53eae358af0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Cleaning\n",
    "def clean_text(text : str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the input text by keeping only alphanumeric characters, spaces, and newline characters (\\n).\n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match alphanumeric characters, spaces, and newlines\n",
    "    pattern = r\"[^a-zA-Z0-9\\s\\n'’]\"\n",
    "    # Substitute all non-matching characters with an empty string\n",
    "    cleaned_text = re.sub(pattern, \"\", text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "data[\"Job Description\"] = data[\"Job Description\"].apply(clean_text)\n",
    "data.head(10)"
   ],
   "id": "3c470f344521b1b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Tokenization\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]+(?:’[A-Za-z]+)?|\\$[\\d\\.]+|\\S+\")\n",
    "\n",
    "data[\"tokens\"] = data[\"Job Description\"].apply(tokenizer.tokenize)\n",
    "\n"
   ],
   "id": "e0aa831b58072e48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = stop_words + [i.replace(\"'\",\"’\") for i in stop_words.copy() if \"'\" in i]\n",
    "\n",
    "data[\"tokens_clean\"] = data[\"tokens\"].apply(lambda x : [i for i in x if i not in stop_words])\n",
    "data[\"tokens_clean\"]"
   ],
   "id": "c30eb7d76966cfd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data[\"lemmas\"] = data[\"tokens_clean\"].apply(lambda x: [lemmatizer.lemmatize(i, pos = \"v\") for i in x])\n",
    "data[\"lemmas\"]"
   ],
   "id": "3ebc26b56f4219d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Combining all\n",
    "\n",
    "data[\"final_text\"] = data[\"lemmas\"].apply(lambda x : \" \".join(x))"
   ],
   "id": "1be74464a0023115",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Machine learning model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data[\"final_text\"]\n",
    "y = data[\"avg_salary\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n",
    "x_training , x_val, y_training, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=123)\n",
    "print(x_training.shape, x_test.shape, x_val.shape)"
   ],
   "id": "14fdcf85dec5b839",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Vectorizing\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "x_train_vect = vect.fit_transform(x_training)\n",
    "x_val_vect = vect.transform(x_val)\n",
    "x_test_vect = vect.transform(x_test)"
   ],
   "id": "7d47dc2732c6d917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Modeling\n",
    "\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "train_pool = Pool(x_train_vect, y_training)\n",
    "eval_pool = Pool(x_val_vect, y_val)\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"iterations\": 10000,\n",
    "    \"depth\": 5,\n",
    "    \"verbose\": False,\n",
    "    \"l2_leaf_reg\": 0.1\n",
    "}\n",
    "model = CatBoostRegressor(**params )\n",
    "model.fit(train_pool, early_stopping_rounds = 100, eval_set = eval_pool)"
   ],
   "id": "7115ddac66768628",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Evaluating model\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "preds = model.predict(x_test_vect)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, preds))\n",
    "print(\"MSE:\", mean_squared_error(y_test, preds))"
   ],
   "id": "f4cf1bcdca80d5d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Analyzing errors\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=y_test, y=preds)\n",
    "sns.lineplot(x=y_test, y=y_test, color = \"red\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Error analysis\")\n",
    "plt.show()"
   ],
   "id": "2592c3d2e0690257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(x_train_vect)\n",
    "\n",
    "mae_cross_validation = []\n",
    "mse_cross_validation = []\n",
    "mae_training_cross_validation = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train_vect)):\n",
    "    model.fit(x_train_vect.toarray()[train_index], y_training.values[train_index], early_stopping_rounds = 100, eval_set = eval_pool)\n",
    "    preds = model.predict(x_train_vect.toarray()[test_index])\n",
    "    preds_training = model.predict(x_train_vect.toarray()[train_index])\n",
    "    mae = mean_absolute_error(y_training.values[test_index], preds)\n",
    "    mae_training = mean_absolute_error(y_training.values[train_index], preds_training)\n",
    "    mae_cross_validation.append(mae)\n",
    "    mae_training_cross_validation.append(mae_training)\n",
    "    mse = mean_squared_error(y_training.values[test_index], preds)\n",
    "    mse_cross_validation.append(mse)"
   ],
   "id": "9a5e7b2aaa5afdeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Analyzing cross validation results\n",
    "\n",
    "print(mae_cross_validation)\n",
    "print(mae_training_cross_validation)"
   ],
   "id": "9fa894e09f7ba753",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    params = {\n",
    "        \"iterations\" : 10000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-3, 10.0, log=True),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \"verbose\": False,  # Disable logging\n",
    "        \"random_seed\": 123,  # Set a fixed random seed for reproducibility\n",
    "    }\n",
    "    model = CatBoostRegressor(**params )\n",
    "    kf = KFold(n_splits=5)\n",
    "    kf.get_n_splits(x_train_vect)\n",
    "\n",
    "    mae_cross_validation = []\n",
    "    mse_cross_validation = []\n",
    "    mae_training_cross_validation = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train_vect)):\n",
    "        model.fit(x_train_vect.toarray()[train_index], y_training.values[train_index], early_stopping_rounds = 100, eval_set = eval_pool)\n",
    "        preds = model.predict(x_train_vect.toarray()[test_index])\n",
    "        preds_training = model.predict(x_train_vect.toarray()[train_index])\n",
    "        mae = mean_absolute_error(y_training.values[test_index], preds)\n",
    "        mae_training = mean_absolute_error(y_training.values[train_index], preds_training)\n",
    "        mae_cross_validation.append(mae)\n",
    "        mae_training_cross_validation.append(mae_training)\n",
    "        mse = mean_squared_error(y_training.values[test_index], preds)\n",
    "        mse_cross_validation.append(mse)\n",
    "\n",
    "    mae_val = np.mean(mae_training_cross_validation)\n",
    "    mae_test = np.mean(mae_cross_validation)\n",
    "    return abs(mae_val - mae_test)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize overfitting\n",
    "study.optimize(objective, n_trials=20)"
   ],
   "id": "afe64ed700ab5029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_params = {\n",
    "    'learning_rate': 0.010004323405318344,\n",
    "     'depth': 4,\n",
    "     'l2_leaf_reg': 0.49946428453675523,\n",
    "     'random_strength': 0.10560927281613303,\n",
    "     'bagging_temperature': 0.27154199895296605,\n",
    "     'border_count': 34,\n",
    "     'min_data_in_leaf': 45,\n",
    "    \"verbose\": False,  # Disable logging\n",
    "    \"random_seed\": 123,\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(x_train_vect)\n",
    "model = CatBoostRegressor(**best_params )\n",
    "mae_cross_validation = []\n",
    "mse_cross_validation = []\n",
    "mae_training_cross_validation = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train_vect)):\n",
    "    model.fit(x_train_vect.toarray()[train_index], y_training.values[train_index], early_stopping_rounds = 100, eval_set = eval_pool)\n",
    "    preds = model.predict(x_train_vect.toarray()[test_index])\n",
    "    preds_training = model.predict(x_train_vect.toarray()[train_index])\n",
    "    mae = mean_absolute_error(y_training.values[test_index], preds)\n",
    "    mae_training = mean_absolute_error(y_training.values[train_index], preds_training)\n",
    "    mae_cross_validation.append(mae)\n",
    "    mae_training_cross_validation.append(mae_training)\n",
    "    mse = mean_squared_error(y_training.values[test_index], preds)\n",
    "    mse_cross_validation.append(mse)"
   ],
   "id": "8fd8242b85db32e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(mae_cross_validation)\n",
    "print(mae_training_cross_validation)\n",
    "\n",
    "model.fit(train_pool, early_stopping_rounds = 100, eval_set = eval_pool)\n",
    "preds = model.predict(x_test_vect)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, preds))\n",
    "print(\"MSE:\", mean_squared_error(y_test, preds))"
   ],
   "id": "e9f0cb2278d4b4b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.scatterplot(x=y_test, y=preds)\n",
    "sns.lineplot(x=y_test, y=y_test, color = \"red\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Error analysis\")\n",
    "plt.show()"
   ],
   "id": "4a505e41680178f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### All in a sklearn pipeline\n",
    "\n",
    "### Preprocessing steps\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class TextPreprocessing(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, text_column, lang = \"english\"):\n",
    "        self.text_column = text_column\n",
    "        self.lang = lang\n",
    "    def fit(self, X, y=None):\n",
    "        def clean_text(text : str) -> str:\n",
    "            \"\"\"\n",
    "            Cleans the input text by keeping only alphanumeric characters, spaces, and newline characters (\\n).\n",
    "            Args:\n",
    "                text (str): The input text to clean.\n",
    "            Returns:\n",
    "                str: The cleaned text.\n",
    "            \"\"\"\n",
    "            # Define a regex pattern to match alphanumeric characters, spaces, and newlines\n",
    "            pattern = r\"[^a-zA-Z0-9\\s\\n'’]\"\n",
    "            # Substitute all non-matching characters with an empty string\n",
    "            cleaned_text = re.sub(pattern, \"\", text)\n",
    "            cleaned_text = cleaned_text.lower()\n",
    "            return cleaned_text\n",
    "        ### Cleaning text\n",
    "\n",
    "        X = X.copy()\n",
    "        X[self.text_column] = X[self.text_column].apply(clean_text)\n",
    "\n",
    "        ### Tokenization\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r\"[A-Za-z]+(?:’[A-Za-z]+)?|\\$[\\d\\.]+|\\S+\")\n",
    "\n",
    "        X[self.text_column] = X[self.text_column].apply(tokenizer.tokenize)\n",
    "\n",
    "        ### Removing stopwords\n",
    "\n",
    "        stop_words = stopwords.words(self.lang)\n",
    "        stop_words = stop_words + [i.replace(\"'\",\"’\") for i in stop_words.copy() if \"'\" in i]\n",
    "        X[self.text_column] = X[self.text_column].apply(lambda x : [i for i in x if i not in stop_words])\n",
    "\n",
    "        ### Lemmatization\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        X[self.text_column] = X[self.text_column].apply(lambda x: [lemmatizer.lemmatize(i, pos = \"v\") for i in x])\n",
    "\n",
    "        ### Joining all together\n",
    "        return X[self.text_column].apply(lambda x : \" \".join(x))\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        return self.fit_transform(X)\n"
   ],
   "id": "8153294812081cad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "text_feature = [\"Job Description\"]\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", TextPreprocessing(text_column = \"Job Description\", lang = \"english\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_transformer, text_feature),\n",
    "        (\"vectorizer\", )\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", TextPreprocessing(text_column = \"Job Description\", lang = \"english\")),\n",
    "        (\"vectorize\", TfidfVectorizer(max_features=1000)),\n",
    "        (\"catboost\", CatBoostRegressor(**best_params))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "clf.fit(train, train[\"avg_salary\"])"
   ],
   "id": "4367fbda2a0bf99d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preds = clf.predict(test)\n",
    "\n",
    "mean_absolute_error(test[\"avg_salary\"], preds)"
   ],
   "id": "d71cb0b28bdd6696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Export as pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(clf, open(\"../myapp/predict_salary/model.pkl\", \"wb\"))"
   ],
   "id": "57609bee4c59b219",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
